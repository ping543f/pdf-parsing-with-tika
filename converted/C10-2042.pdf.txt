



















































Conundrums in Unsupervised Keyphrase Extraction: Making Sense of the State-of-the-Art


Coling 2010: Poster Volume, pages 365–373,
Beijing, August 2010

Conundrums in Unsupervised Keyphrase Extraction:
Making Sense of the State-of-the-Art

Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute

University of Texas at Dallas
{saidul,vince}@hlt.utdallas.edu

Abstract

State-of-the-art approaches for unsuper-
vised keyphrase extraction are typically
evaluated on a single dataset with a single
parameter setting. Consequently, it is un-
clear how effective these approaches are
on a new dataset from a different domain,
and how sensitive they are to changes in
parameter settings. To gain a better under-
standing of state-of-the-art unsupervised
keyphrase extraction algorithms, we con-
duct a systematic evaluation and analysis
of these algorithms on a variety of stan-
dard evaluation datasets.

1 Introduction

The keyphrases for a given document refer to a
group of phrases that represent the document. Al-
though we often come across texts from different
domains such as scientific papers, news articles
and blogs, which are labeled with keyphrases by
the authors, a large portion of the Web content re-
mains untagged. While keyphrases are excellent
means for providing a concise summary of a doc-
ument, recent research results have suggested that
the task of automatically identifying keyphrases
from a document is by no means trivial. Re-
searchers have explored both supervised and un-
supervised techniques to address the problem of
automatic keyphrase extraction. Supervised meth-
ods typically recast this problem as a binary clas-
sification task, where a model is trained on anno-
tated data to determine whether a given phrase is
a keyphrase or not (e.g., Frank et al. (1999), Tur-
ney (2000; 2003), Hulth (2003), Medelyan et al.
(2009)). A disadvantage of supervised approaches

is that they require a lot of training data and yet
show bias towards the domain on which they are
trained, undermining their ability to generalize
well to new domains. Unsupervised approaches
could be a viable alternative in this regard.

The unsupervised approaches for keyphrase ex-
traction proposed so far have involved a number
of techniques, including language modeling (e.g.,
Tomokiyo and Hurst (2003)), graph-based rank-
ing (e.g., Zha (2002), Mihalcea and Tarau (2004),
Wan et al. (2007), Wan and Xiao (2008), Liu
et al. (2009a)), and clustering (e.g., Matsuo and
Ishizuka (2004), Liu et al. (2009b)). While these
methods have been shown to work well on a par-
ticular domain of text such as short paper abstracts
and news articles, their effectiveness and portabil-
ity across different domains have remained an un-
explored issue. Worse still, each of them is based
on a set of assumptions, which may only hold for
the dataset on which they are evaluated.

Consequently, we have little understanding of
how effective the state-of the-art systems would be
on a completely new dataset from a different do-
main. A few questions arise naturally. How would
these systems perform on a different dataset with
their original configuration? What could be the
underlying reasons in case they perform poorly?
Is there any system that can generalize fairly well
across various domains?

We seek to gain a better understanding of the
state of the art in unsupervised keyphrase ex-
traction by examining the aforementioned ques-
tions. More specifically, we compare five unsu-
pervised keyphrase extraction algorithms on four
corpora with varying domains and statistical char-
acteristics. These algorithms represent the ma-

365



jor directions in this research area, including Tf-
Idf and four recently proposed systems, namely,
TextRank (Mihalcea and Tarau, 2004), SingleR-
ank (Wan and Xiao, 2008), ExpandRank (Wan
and Xiao, 2008), and a clustering-based approach
(Liu et al., 2009b). Since none of these systems
(except TextRank) are publicly available, we re-
implement all of them and make them freely avail-
able for research purposes.1 To our knowledge,
this is the first attempt to compare the perfor-
mance of state-of-the-art unsupervised keyphrase
extraction systems on multiple datasets.

2 Corpora

Our four evaluation corpora belong to different
domains with varying document properties. Ta-
ble 1 provides an overview of each corpus.

The DUC-2001 dataset (Over, 2001), which is
a collection of 308 news articles, is annotated by
Wan and Xiao (2008). We report results on all 308
articles in our evaluation.

The Inspec dataset is a collection of 2,000 ab-
stracts from journal papers including the paper ti-
tle. Each document has two sets of keyphrases as-
signed by the indexers: the controlled keyphrases,
which are keyphrases that appear in the In-
spec thesaurus; and the uncontrolled keyphrases,
which do not necessarily appear in the thesaurus.
This is a relatively popular dataset for automatic
keyphrase extraction, as it was first used by Hulth
(2003) and later by Mihalcea and Tarau (2004)
and Liu et al. (2009b). In our evaluation, we use
the set of 500 abstracts designated by these previ-
ous approaches as the test set and its set of uncon-
trolled keyphrases. Note that the average docu-
ment length for this dataset is the smallest among
all our datasets.

The NUS Keyphrase Corpus (Nguyen and
Kan, 2007) includes 211 scientific conference pa-
pers with lengths between 4 to 12 pages. Each
paper has one or more sets of keyphrases assigned
by its authors and other annotators. We use all the
211 papers in our evaluation. Since the number
of annotators can be different for different docu-
ments and the annotators are not specified along
with the annotations, we decide to take the union

1See http://www.hlt.utdallas.edu/
˜saidul/code.html for details.

of all the gold standard keyphrases from all the
sets to construct one single set of annotation for
each paper. As Table 1 shows, each NUS pa-
per, both in terms of the average number of to-
kens (8291) and candidate phrases (2027) per pa-
per, is more than five times larger than any doc-
ument from any other corpus. Hence, the num-
ber of candidate keyphrases that can be extracted
is potentially large, making this corpus the most
challenging of the four.

Finally, the ICSI meeting corpus (Janin et al.,
2003), which is annotated by Liu et al. (2009a),
includes 161 meeting transcriptions. Following
Liu et al., we remove topic segments marked as
’chitchat’ and ’digit’ from the dataset and use all
the remaining segments for evaluation. Each tran-
script contains three sets of keyphrases produced
by the same three human annotators. Since it is
possible to associate each set of keyphrases with
its annotator, we evaluate each system on this
dataset three times, once for each annotator, and
report the average score. Unlike the other three
datasets, the gold standard keys for the ICSI cor-
pus are mostly unigrams.

3 Unsupervised Keyphrase Extractors

A generic unsupervised keyphrase extraction sys-
tem typically operates in three steps (Section 3.1),
which will help understand the unsupervised sys-
tems explained in Section 3.2.

3.1 Generic Keyphrase Extractor

Step 1: Candidate lexical unit selection The
first step is to filter out unnecessary word to-
kens from the input document and generate a list
of potential keywords using heuristics. Com-
monly used heuristics include (1) using a stop
word list to remove non-keywords (e.g., Liu et al.
(2009b)) and (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to
be considered candidate keywords (Mihalcea and
Tarau (2004), Liu et al. (2009a), Wan and Xiao
(2008)). In all of our experiments, we follow Wan
and Xiao (2008) and select as candidates words
with the following Penn Treebank tags: NN, NNS,
NNP, NNPS, and JJ, which are obtained using the
Stanford POS tagger (Toutanova and Manning,
2000).

366



Corpora
DUC-2001 Inspec NUS ICSI

Type News articles Paper abstracts Full papers Meeting transcripts
# Documents 308 500 211 161

# Tokens/Document 876 134 8291 1611
# Candidate words/Document 312 57 3271 453

# Candidate phrases/Document 207 34 2027 296
# Tokens/Candidate phrase 1.5 1.7 1.6 1.5

# Gold keyphrases 2484 4913 2327 582
# Gold keyphrases/Document 8.1 9.8 11.0 3.6

U/B/T/O distribution (%) 17/61/18/4 13/53/25/9 27/50/16/7 68/29/2/1
# Tokens/Gold keyphrase 2.1 2.3 2.1 1.3

Table 1: Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence
of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase. The

U/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other

higher order n-grams.

Step 2: Lexical unit ranking Once the can-
didate list is generated, the next task is to rank
these lexical units. To accomplish this, it is nec-
essary to build a representation of the input text
for the ranking algorithm. Depending on the un-
derlying approach, each candidate word is repre-
sented by its syntactic and/or semantic relation-
ship with other candidate words. The relationship
can be defined using co-occurrence statistics, ex-
ternal resources (e.g., neighborhood documents,
Wikipedia), or other syntactic clues.

Step 3: Keyphrase formation In the final step,
the ranked list of candidate words is used to form
keyphrases. A candidate phrase, typically a se-
quence of nouns and adjectives, is selected as a
keyphrase if (1) it includes one or more of the
top-ranked candidate words (Mihalcea and Tarau
(2004), Liu et al. (2009b)), or (2) the sum of the
ranking scores of its constituent words makes it a
top scoring phrase (Wan and Xiao, 2008).

3.2 The Five Keyphrase Extractors

As mentioned above, we re-implement five unsu-
pervised approaches for keyphrase extraction. Be-
low we provide a brief overview of each system.

3.2.1 Tf-Idf

Tf-Idf assigns a score to each term t in a doc-
ument d based on t’s frequency in d (term fre-
quency) and how many other documents include
t (inverse document frequency) and is defined as:

tfidft = tft × log(D/Dt) (1)

where D is the total number of documents and Dt
is the number of documents containing t.

Given a document, we first compute the Tf-
Idf score of each candidate word (see Step 1 of
the generic algorithm). Then, we extract all the
longest n-grams consisting of candidate words
and score each n-gram by summing the Tf-Idf
scores of its constituent unigrams. Finally, we out-
put the top N n-grams as keyphrases.

3.2.2 TextRank

In the TextRank algorithm (Mihalcea and Ta-
rau, 2004), a text is represented by a graph. Each
vertex corresponds to a word type. A weight,
wij , is assigned to the edge connecting the two
vertices, vi and vj , and its value is the number
of times the corresponding word types co-occur
within a window of W words in the associated
text. The goal is to (1) compute the score of each
vertex, which reflects its importance, and then (2)
use the word types that correspond to the highest-
scored vertices to form keyphrases for the text.
The score for vi, S(vi), is initialized with a de-
fault value and is computed in an iterative manner
until convergence using this recursive formula:

S(vi) = (1− d) + d×
∑

vjǫAdj(vi)

wji∑
vkǫAdj(vj)

wjk
S(vj)

(2)

where Adj(vi) denotes vi’s neighbors and d is the
damping factor set to 0.85 (Brin and Page, 1998).
Intuitively, a vertex will receive a high score if it
has many high-scored neighbors. As noted before,
after convergence, the T% top-scored vertices are

367



selected as keywords. Adjacent keywords are then
collapsed and output as a keyphrase.

According to Mihalcea and Tarau (2004), Tex-
tRank’s best score on the Inspec dataset is
achieved when only nouns and adjectives are used
to create a uniformly weighted graph for the text
under consideration, where an edge connects two
word types only if they co-occur within a window
of two words. Hence, our implementation of Tex-
tRank follows this configuration.

3.2.3 SingleRank

SingleRank (Wan and Xiao, 2008) is essen-
tially a TextRank approach with three major dif-
ferences. First, while each edge in a TextRank
graph (in Mihalcea and Tarau’s implementation)
has the same weight, each edge in a SingleRank
graph has a weight equal to the number of times
the two corresponding word types co-occur. Sec-
ond, while in TextRank only the word types that
correspond to the top-ranked vertices can be used
to form keyphrases, in SingleRank, we do not fil-
ter out any low-scored vertices. Rather, we (1)
score each candidate keyphrase, which can be any
longest-matching sequence of nouns and adjec-
tives in the text under consideration, by summing
the scores of its constituent word types obtained
from the SingleRank graph, and (2) output the N
highest-scored candidates as the keyphrases for
the text. Finally, SingleRank employs a window
size of 10 rather than 2.

3.2.4 ExpandRank

ExpandRank (Wan and Xiao, 2008) is a
TextRank extension that exploits neighborhood
knowledge for keyphrase extraction. For a given
document d, the approach first finds its k near-
est neighboring documents from the accompany-
ing document collection using a similarity mea-
sure (e.g., cosine similarity). Then, the graph for
d is built using the co-occurrence statistics of the
candidate words collected from the document it-
self and its k nearest neighbors.

Specifically, each document is represented by
a term vector where each vector dimension cor-
responds to a word type present in the document
and its value is the Tf-Idf score of that word type
for that document. For a given document d0, its k

nearest neighbors are identified, and together they
form a larger document set of k+1 documents,
D = {d0, d1, d2, ..., dk}. Given this document
set, a graph is constructed, where each vertex cor-
responds to a candidate word type in D, and each
edge connects two vertices vi and vj if the corre-
sponding word types co-occur within a window of
W words in the document set. The weight of an
edge, w(vi, vj), is computed as follows:

w(vi, vj) =
∑

dkǫD

sim(d0, dk)× freqdk(vi, vj) (3)

where sim(d0, dk) is the cosine similarity be-
tween d0 and dk, and freqdk(vi, vj) is the co-
occurrence frequency of vi and vj in document dk.
Once the graph is constructed, the rest of the pro-
cedure is identical to SingleRank.

3.2.5 Clustering-based Approach

Liu et al. (2009b) propose to cluster candidate
words based on their semantic relationship to en-
sure that the extracted keyphrases cover the en-
tire document. The objective is to have each clus-
ter represent a unique aspect of the document and
take a representative word from each cluster so
that the document is covered from all aspects.

More specifically, their algorithm (henceforth
referred to as KeyCluster) first filters out the stop
words from a given document and treats the re-
maining unigrams as candidate words. Second,
for each candidate, its relatedness with another
candidate is computed by (1) counting how many
times they co-occur within a window of size W
in the document and (2) using Wikipedia-based
statistics. Third, candidate words are clustered
based on their relatedness with other candidates.
Three clustering algorithms are used of which
spectral clustering yields the best score. Once
the clusters are formed, one representative word,
called an exemplar term, is picked from each clus-
ter. Finally, KeyCluster extracts from the docu-
ment all the longest n-grams starting with zero
or more adjectives and ending with one or more
nouns, and if such an n-gram includes one or more
exemplar words, it is selected as a keyphrase. As
a post-processing step, a frequent word list gener-
ated from Wikipedia is used to filter out the fre-
quent unigrams that are selected as keyphrases.

368



4 Evaluation

4.1 Experimental Setup

TextRank and SingleRank setup Following
Mihalcea and Tarau (2004) and Wan and Xiao
(2008), we set the co-occurrence window size for
TextRank and SingleRank to 2 and 10, respec-
tively, as these parameter values have yielded the
best results for their evaluation datasets.

ExpandRank setup Following Wan and Xiao
(2008), we find the 5 nearest neighbors for each
document from the remaining documents in the
same corpus. The other parameters are set in the
same way as in SingleRank.

KeyCluster setup As argued by Liu et al.
(2009b), Wikipedia-based relatedness is computa-
tionally expensive to compute. As a result, we fol-
low them by computing the co-occurrence-based
relatedness instead, using a window of size 10.
Then, we cluster the candidate words using spec-
tral clustering, and use the frequent word list that
they generously provided us to post-process the
resulting keyphrases by filtering out those that are
frequent unigrams.

4.2 Results and Discussion

In an attempt to gain a better insight into the
five unsupervised systems, we report their perfor-
mance in terms of precision-recall curves for each
of the four datasets (see Figure 1). This contrasts
with essentially all previous work, where the per-
formance of a keyphrase extraction system is re-
ported in terms of an F-score obtained via a par-
ticular parameter setting on a particular dataset.
We generate the curves for each system as fol-
lows. For Tf-Idf, SingleRank, and ExpandRank,
we vary the number of keyphrases, N , predicted
by each system. For TextRank, instead of vary-
ing the number of predicted keyphrases, we vary
T , the percentage of top-scored vertices (i.e., un-
igrams) that are selected as keywords at the end
of the ranking step. The reason is that TextRank
only imposes a ranking on the unigrams but not
on the keyphrases generated from the high-ranked
unigrams. For KeyCluster, we vary the number
of clusters produced by spectral clustering rather
than the number of predicted keyphrases, again
because KeyCluster does not impose a ranking on

the resulting keyphrases. In addition, to give an
estimate of how each system performs in terms of
F-score, we also plot curves corresponding to dif-
ferent F-scores in these graphs.

Tf-Idf Consistent with our intuition, the preci-
sion of Tf-Idf drops as recall increases. Although
it is the simplest of the five approaches, Tf-Idf is
the best performing system on all but the Inspec
dataset, where TextRank and KeyCluster beat Tf-
Idf on just a few cases. It clearly outperforms all
other systems for NUS and ICSI.

TextRank The TextRank curves show a differ-
ent progression than Tf-Idf: precision does not
drop as much when recall increases. For instance,
in case of DUC and ICSI, precision is not sensi-
tive to changes in recall. Perhaps somewhat sur-
prisingly, its precision increases with recall for In-
spec, allowing it to even reach a point (towards
the end of the curve) where it beats Tf-Idf. While
additional experiments are needed to determine
the reason for this somewhat counter-intuitive re-
sult, we speculate that this may be attributed to
the fact that the TextRank curves are generated
by progressively increasing T (i.e., the percent-
age of top-ranked vertices/unigrams that are used
to generate keyphrases) rather than the number of
predicted keyphrases, as mentioned before. In-
creasing T does not necessarily imply an increase
in the number of predicted keyphrases, however.
To see the reason, consider an example in which
we want TextRank to extract the keyphrase “ad-
vanced machine learning” for a given document.
Assume that TextRank ranks the unigrams “ad-
vanced”, “learning”, and “machine” first, second,
and third, respectively in its ranking step. When
T = 2n , where n denotes the total number of
candidate unigrams, only the two highest-ranked
unigrams (i.e., “advanced” and “learning”) can
be used to form keyphrases. This implies that
“advanced” and “learning” will each be predicted
as a keyphrase, but “advanced machine learning”
will not. However, when T = 3n , all three un-
igrams can be used to form a keyphrase, and
since TextRank collapses unigrams adjacent to
each other in the text to form a keyphrase, it will
correctly predict “advanced machine learning” as
a keyphrase. Note that as we increase T from 2n
to 3n , recall increases, and so does precision, since

369



 0

 10

 20

 30

 40

 50

 0  20  40  60  80  100

Pr
ec

is
io

n 
(%

)

Recall (%)

F=10

F=20

F=30

F=40

Tf-Idf
TextRank

SingleRank
ExpandRank

KeyCluster

(a) DUC

 0

 10

 20

 30

 40

 50

 0  20  40  60  80  100

Pr
ec

is
io

n 
(%

)

Recall (%)

F=10

F=20

F=30

F=40

Tf-Idf
TextRank

SingleRank
ExpandRank

KeyCluster

(b) Inspec

 0

 2

 4

 6

 8

 10

 0  20  40  60  80  100

Pr
ec

is
io

n 
(%

)

Recall (%)

F=10

Tf-Idf
TextRank

SingleRank
ExpandRank

KeyCluster

(c) NUS

 0

 2

 4

 6

 8

 10

 0  20  40  60  80  100

Pr
ec

is
io

n 
(%

)

Recall (%)

F=10

Tf-Idf
TextRank

SingleRank
ExpandRank

KeyCluster

(d) ICSI

Figure 1: Precision-recall curves for all four datasets

“advanced” and “learning” are now combined to
form one keyphrase (and hence the number of pre-
dicted keyphrases decreases). In other words, it
is possible to see a simultaneous rise in precision
and recall in a TextRank curve. A natural ques-
tion is: why does is happen only for Inspec but
not the other datasets? The reason could be at-
tributed to the fact that Inspec is composed of ab-
stracts: since the number of keyphrases that can be
generated from these short documents is relatively
small, precision may not drop as severely as with
the other datasets even when all of the unigrams
are used to form keyphrases.

On average, TextRank performs much worse

compared to Tf-Idf. The curves also prove Tex-
tRank’s sensitivity to T on Inspec, but not on the
other datasets. This certainly gives more insight
into TextRank since it was evaluated on Inspec
only for T=33% by Mihalcea and Tarau (2004).

SingleRank SingleRank, which is supposed to
be a simple variant of TextRank, surprisingly ex-
hibits very different performance. First, it shows
a more intuitive nature: precision drops as recall
increases. Second, SingleRank outperforms Tex-
tRank by big margins on all the datasets. Later,
we will examine which of the differences between
them is responsible for the differing performance.

370



DUC Inspec NUS ICSI
Parameter F Parameter F Parameter F Parameter F

Tf-Idf N = 14 27.0 N = 14 36.3 N = 60 6.6 N = 9 12.1
TextRank T = 100% 9.7 T = 100% 33.0 T = 5% 3.2 T = 25% 2.7

SingleRank N = 16 25.6 N = 15 35.3 N = 190 3.8 N = 50 4.4
ExpandRank N = 13 26.9 N = 15 35.3 N = 177 3.8 N = 51 4.3
KeyCluster m = 0.9n 14.0 m = 0.9n 40.6 m = 0.25n 1.7 m = 0.9n 3.2

Table 2: Best parameter settings. N is the number of predicted keyphrases, T is the percentage of vertices selected as
keywords in TextRank, m is the number of clusters in KeyCluster, expressed in terms of n, the fraction of candidate words.

ExpandRank Consistent with Wan and Xiao
(2008), ExpandRank beats SingleRank on DUC
when a small number of phrases are predicted, but
their difference diminishes as more phrases are
predicted. On the other hand, their performance
is indistinguishable from each other on the other
three datasets. A natural question is: why does
ExpandRank improve over SingleRank only for
DUC but not for the other datasets? To answer
this question, we look at the DUC articles and
find that in many cases, the 5-nearest neighbors
of a document are on the same topic involving the
same entities as the document itself, presumably
because many of these news articles are simply
updated versions of an evolving event. Conse-
quently, the graph built from the neighboring doc-
uments is helpful for predicting the keyphrases of
the given document. Such topic-wise similarity
among the nearest documents does not exist in the
other datasets, however.

KeyCluster As in TextRank, KeyCluster does
not always yield a drop in precision as recall im-
proves. This, again, may be attributed to the fact
that the KeyCluster curves are generated by vary-
ing the number of clusters rather than the num-
ber of predicted keyphrases, as well as the way
keyphrases are formed from the exemplars. An-
other reason is that the frequent Wikipedia uni-
grams are excluded during post-processing, mak-
ing KeyCluster more resistant to precision drops.
Overall, KeyCluster performs slightly better than
TextRank on DUC and ICSI, yields the worst per-
formance on NUS, and scores the best on Inspec
when the number of clusters is high. These results
seem to suggest that KeyCluster works better if
more clusters are used.

Best parameter settings Table 2 shows for each
system the parameter values yielding the best F-
score on each dataset. Two points deserve men-

tion. First, in comparison to SingleRank and
ExpandRank, Tf-Idf outputs fewer keyphrases to
achieve its best F-score on most datasets. Second,
the systems output more keyphrases on NUS than
on other datasets to achieve their best F-scores
(e.g., 60 for Tf-Idf, 190 for SingleRank, and 177
for ExpandRank). This can be attributed in part to
the fact that the F-scores on NUS are low for all
the systems and exhibit only slight changes as we
output more phrases.

Our re-implementations Do our duplicated
systems yield scores that match the original
scores? Table 3 sheds light on this question.

First, consider KeyCluster, where our score
lags behind the original score by approximately
5%. An examination of Liu et al.’s (2009b) re-
sults reveals a subtle caveat in keyphrase extrac-
tion evaluations. In Inspec, not all gold-standard
keyphrases appear in their associated document,
and as a result, none of the five systems we con-
sider in this paper can achieve a recall of 100.
While Mihalcea and Tarau (2004) and our re-
implementations use all of these gold-standard
keyphrases in our evaluation, Hulth (2003) and
Liu et al. address this issue by using as gold-
standard keyphrases only those that appear in the
corresponding document when computing recall.2

This explains why our KeyCluster score (38.9) is
lower than the original score (43.6). If we fol-
low Liu et al.’s way of computing recall, our re-
implementation score goes up to 42.4, which lags
behind their score by only 1.2.

Next, consider TextRank, where our score lags
behind Mihalcea and Tarau’s original score by
more than 25 points. We verified our implemen-
tation against a publicly available implementation

2As a result, Liu et al. and Mihalcea and Tarau’s scores
are not directly comparable, but Liu et al. did not point this
out while comparing scores in their paper.

371



Dataset F-score
Original Ours

Tf-Idf DUC 25.4 25.7
TextRank Inspec 36.2 10.0

SingleRank DUC 27.2 24.9
ExpandRank DUC 31.7 26.4
KeyCluster Inspec 43.6 38.9

Table 3: Original vs. re-implementation scores

of TextRank3, and are confident that our imple-
mentation is correct. It is also worth mentioning
that using our re-implementation of SingleRank,
we are able to match the best scores reported by
Mihalcea and Tarau (2004) on Inspec.

We score 2 and 5 points less than Wan and
Xiao’s (2008) implementations of SingleRank and
ExpandRank, respectively. We speculate that doc-
ument pre-processing (e.g., stemming) has con-
tributed to the discrepancy, but additional exper-
iments are needed to determine the reason.

SingleRank vs. TextRank Figure 1 shows that
SingleRank behaves very differently from Tex-
tRank. As mentioned in Section 3.2.3, the two
algorithms differ in three major aspects. To de-
termine which aspect is chiefly responsible for the
large difference in their performance, we conduct
three “ablation” experiments. Each experiment
modifies exactly one of these aspects in SingleR-
ank so that it behaves like TextRank, effectively
ensuring that the two algorithms differ only in the
remaining two aspects. More specifically, in the
three experiments, we (1) change SingleRank’s
window size to 2, (2) build an unweighted graph
for SingleRank, and (3) incorporate TextRank’s
way of forming keyphrases into SingleRank, re-
spectively. Figure 2 shows the resultant curves
along with the SingleRank and TextRank curves
on Inspec taken from Figure 1b. As we can see,
the way of forming phrases, rather than the win-
dow size or the weight assignment method, has
the largest impact on the scores. In fact, after in-
corporating TextRank’s way of forming phrases,
SingleRank exhibits a remarkable drop in perfor-
mance, yielding a curve that resembles the Tex-
tRank curve. Also note that SingleRank achieves
better recall values than TextRank. To see the rea-
son, recall that TextRank requires that every word
of a gold keyphrase must appear among the top-

3http://github.com/sharethis/textrank

 0

 10

 20

 30

 40

 50

 0  20  40  60  80  100

Pr
ec

is
io

n 
(%

)

Recall (%)

F=10

F=20

F=30

F=40

SingleRank
SingleRank+Window size=2

SingleRank+Unweighted
SingleRank+TextRank phrases

TextRank

Figure 2: Ablation results for SingleRank on In-
spec

ranked unigrams. This is a fairly strict criterion,
especially in comparison to SingleRank, which
does not require all unigrams of a gold keyphrase
to be present in the top-ranked list. We observe
similar trends for the other datasets.

5 Conclusions

We have conducted a systematic evaluation of five
state-of-the-art unsupervised keyphrase extraction
algorithms on datasets from four different do-
mains. Several conclusions can be drawn from
our experimental results. First, to fully under-
stand the strengths and weaknesses of a keyphrase
extractor, it is essential to evaluate it on multi-
ple datasets. In particular, evaluating it on a sin-
gle dataset has proven inadequate, as good per-
formance can sometimes be achieved due to cer-
tain statistical characteristics of a dataset. Sec-
ond, as demonstrated by our experiments with
TextRank and SingleRank, post-processing steps
such as the way of forming keyphrases can have
a large impact on the performance of a keyphrase
extractor. Hence, it may be worthwhile to investi-
gate alternative methods for extracting candidate
keyphrases (e.g., Kumar and Srinathan (2008),
You et al. (2009)). Finally, despite the large
amount of recent work on unsupervised keyphrase
extractor, our results indicated that Tf-Idf remains
a strong baseline, offering very robust perfor-
mance across different datasets.

372



Acknowledgments

We thank the three anonymous reviewers for their
comments. Many thanks to Anette Hulth and
Yang Liu for providing us with the Inspec and
ICSI datasets; Rada Mihalcea, Paco Nathan, and
Xiaojun Wan for helping us understand their al-
gorithms/implementations; and Peng Li for pro-
viding us with the frequent word list that he and
his co-authors used in their paper. This work was
supported in part by NSF Grant IIS-0812261.

References

Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1–7):107–117.

Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668–673.

Hulth, Anette. 2003. Improved automatic keyword
extraction given more linguistic knowledge. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
216–223.

Janin, Adam, Don Baron, Jane Edwards, Dan El-
lis, David Gelbart, Nelson Morgan, Barbara Pe-
skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stol-
cke, and Chuck Wooters. 2003. The ICSI meeting
corpus. In Proceedings of 2003 IEEE Conference
on Acoustics, Speech, and Signal Processing, pages
364–367.

Kumar, Niraj and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the Eighth ACM Symposium on Document Engi-
neering, pages 199–208.

Liu, Feifan, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic
keyword extraction using meeting transcripts. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 620–628.

Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257–266.

Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157–
169.

Medelyan, Olena, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318–1327.

Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404–411.

Nguyen, Thuy Dung and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317–326.

Over, Paul. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.

Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.

Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language processing
and Very Large Corpora, pages 63–70.

Turney, Peter. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval,
2:303–336.

Turney, Peter. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434–439.

Wan, Xiaojun and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855–860.

Wan, Xiaojun, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552–559.

You, Wei, Dominique Fontaine, and Jean-Paul Barthès.
2009. Automatic keyphrase extraction with a re-
fined candidate set. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576–579.

Zha, Hongyuan. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings of
the 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113–120.

373


